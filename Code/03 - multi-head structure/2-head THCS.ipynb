{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THCS\n",
    "- 2 head\n",
    "- multi-head structure evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy.stats as st\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from redo_handy_utils import *\n",
    "from redo_utils import *\n",
    "from dl_utils import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# tensorboard path\n",
    "basepath = '/home/wengqy20/ML - work'\n",
    "model_path = '06 - PINN/07 - code github/03 - multi-head structure/saved model'\n",
    "log_path = '06 - PINN/07 - code github/03 - multi-head structure/tensorboard'\n",
    "date_suf = 'THCS - ' + datetime.now().strftime(\"%Y%m%d - %H%M%S\")\n",
    "writer = SummaryWriter(os.path.join(basepath, log_path,  date_suf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        '''\n",
    "        Args:\n",
    "        layers  ::  array-like, number of neurons in each layer of the neural network\n",
    "        \n",
    "        '''\n",
    "        super(DNN, self).__init__()\n",
    "        # Network depth\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # activation function\n",
    "        # self.activation = torch.nn.ReLU\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        # hidden layer\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "        # output layer\n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(layerDict).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "def initialize_net(net):\n",
    "    net_name = net.__class__.__name__                               # 2\n",
    "    if net_name.find('layer') != -1:                                              \n",
    "        nn.init.xavier_normal_(net.weight)\n",
    "        nn.init.xavier_normal_(net.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THCS\n",
    "class THCS():\n",
    "    def __init__(self, X_eq, U_eq, X_gap, U_gap, X_nw, T_nw, model_s, model_u, model_T, prop_net, **kargs):\n",
    "        '''\n",
    "        Args:\n",
    "        X_eq     ::  2d array, coordinate of scarce samples used for label loss\n",
    "        U_eq     ::  2d array, physical parameters of scarce samples used for label loss\n",
    "        X_gap    ::  2d array, coordinate of scarce samples used for equation loss\n",
    "        U_gap    ::  2d array, physical parameters of scarce samples used for equation loss\n",
    "        X_nw     ::  2d array, coordinate of scarce samples used for wall-temperature loss\n",
    "        T_nw     ::  2d array, temperature of scarce samples used for wall-temperature loss\n",
    "        model    ::  pkl, THCS model\n",
    "        prop_net ::  pkl, MLP for thermal property mapping\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.X_eq = torch.tensor(X_eq, requires_grad=True).float().to(device)\n",
    "        self.U_eq = torch.tensor(U_eq, requires_grad=True).float().to(device)\n",
    "        self.X_gap = torch.tensor(X_gap, requires_grad=True).float().to(device)\n",
    "        self.U_gap = torch.tensor(U_gap, requires_grad=True).float().to(device)\n",
    "        self.X_nw = torch.tensor(X_nw, requires_grad=True).float().to(device)\n",
    "        self.T_nw = torch.tensor(T_nw, requires_grad=True).float().to(device)\n",
    "        self.dnn_s = model_s\n",
    "        self.dnn_u = model_u\n",
    "        self.dnn_T = model_T\n",
    "        self.prop_net = prop_net\n",
    "        self.lb_T = kargs['T_lb']  # lower bound for the input of property network\n",
    "        self.ub_T = kargs['T_ub']  # upper bound for the input of property network\n",
    "        self.prop_cof = kargs['prop_cof']  # coefficient for property network\n",
    "        self.T_pc = kargs['Tpc']  # pseudo-critical temperature\n",
    "        self.ub = torch.tensor(kargs['ub'], requires_grad=True).float().to(device)  # lower bound for the input of main network\n",
    "        self.lb = torch.tensor(kargs['lb'], requires_grad=True).float().to(device)  # upper bound for the input of main network\n",
    "        self.loss_weight = kargs['loss_weight']  # weights of loss function\n",
    "\n",
    "        self.u_eq = self.U_eq[:, 0:1]\n",
    "        self.v_eq = self.U_eq[:, 1:2]\n",
    "        self.T_eq = self.U_eq[:, 2:3]\n",
    "        self.p_eq = self.U_eq[:, 7:8]\n",
    "        self.data_0d = kargs['data_0d']\n",
    "        self.fluid = 'CO2'\n",
    "\n",
    "        # Dimensionless number\n",
    "        self.T0 = self.data_0d.loc[0, 'T0']\n",
    "        self.L0 = self.data_0d.loc[0, 'L0']\n",
    "        self.U0 = self.data_0d.loc[0, 'U0']\n",
    "        self.D0 = self.data_0d.loc[0, 'D0']\n",
    "        self.M0 = self.data_0d.loc[0, 'M0']\n",
    "        self.k0 = self.data_0d.loc[0, 'k0']\n",
    "        self.Cp0 = self.data_0d.loc[0, 'Cp0']\n",
    "        self.hpc = self.data_0d.loc[0, 'hpc'] /self.Cp0/self.T0\n",
    "        self.prop_scl = [self.prop_cof[0]/self.D0, self.prop_cof[1]/self.M0, self.prop_cof[2]/self.k0, self.prop_cof[3]/self.Cp0/self.T0]  # scale factor for property network\n",
    "\n",
    "        self.gx = -9.8  # upward flow\n",
    "        self.Fr = self.U0**2 / (self.gx * self.L0)\n",
    "        self.Re = self.D0 * self.U0 * self.L0 / self.M0\n",
    "        self.Pr = self.M0 * self.Cp0 * self.k0\n",
    "       \n",
    "        self.iter = 0\n",
    "\n",
    "    # physical field prediction by main network\n",
    "    def net_pd(self, x, r):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x   ::  array-like or float, dimensionless x-coordinate\n",
    "        r   ::  array-like or float, dimensionless r-coordinate\n",
    "\n",
    "        Return:\n",
    "        u   ::  array-like or float, dimensionless streamwise velocity\n",
    "        v   ::  array-like or float, dimensionless radial velocity\n",
    "        p   ::  array-like or float, dimensionless hydraulic pressure\n",
    "        T   ::  array-like or float, dimensionless Temperature\n",
    "\n",
    "        \"\"\"\n",
    "        x_scl = 2.0*(x - self.lb[0])/(self.ub[0] - self.lb[0]) - 1.0\n",
    "        r_scl = 2.0*(r - self.lb[1])/(self.ub[1] - self.lb[1]) - 1.0\n",
    "        X_scl = torch.concat((x_scl, r_scl), dim=1)\n",
    "        field_s = self.dnn_s(X_scl)\n",
    "        field_u = self.dnn_u(field_s[:, :24])\n",
    "        field_T = self.dnn_T(field_s[:, 24:])\n",
    "        u_d, v_d, p, Mt = field_u[:, 0:1], field_u[:, 1:2], field_u[:, 2:3], field_u[:, 3:4]\n",
    "        T, kt = field_T[:, 0:1], field_T[:, 1:2]\n",
    "        # hard manner for no-slip condition\n",
    "        u = (1-r)*u_d\n",
    "        v = (1-r)*v_d\n",
    "        return u, v, p, T, Mt, kt\n",
    "    \n",
    "    # thermal property prediction by property network\n",
    "    def prop_pd(self, T):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        T   ::  array-like or float, dimensionless temperature\n",
    "\n",
    "        Return:\n",
    "        D   ::  array-like or float, dimensionless density\n",
    "        M   ::  array-like or float, dimensionless viscosity\n",
    "        k   ::  array-like or float, dimensionless conductivity\n",
    "        h   ::  array-like or float, dimensionless enthalpy\n",
    "\n",
    "        \"\"\"\n",
    "        T_dim = self.T0 * T  # Dimensionalization\n",
    "        T_scl = 2.0*(T_dim - self.lb_T)/(self.ub_T - self.lb_T) - 1.0\n",
    "        prop_dim = self.prop_net(T_scl)\n",
    "        D, M, k, h = self.prop_scl[0]*prop_dim[:, 0:1], self.prop_scl[1]*prop_dim[:, 1:2], self.prop_scl[2]*prop_dim[:, 2:3], self.prop_scl[3]*prop_dim[:, 3:4] - self.hpc\n",
    "        \n",
    "        return D, M, k, h\n",
    "    \n",
    "    # heat flux prediction\n",
    "    def qw_pd(self, T, r):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        T   ::  array-like or float, dimensionless temperature\n",
    "        r   ::  array-like or float, dimensionless r-coordinate\n",
    "\n",
    "        Return:\n",
    "        qw  ::  array-like or float, dimensionless heat flux\n",
    "\n",
    "        \"\"\"\n",
    "        D, M, k, h = self.prop_pd(T)\n",
    "        T_y = torch.autograd.grad(\n",
    "            T, r, \n",
    "            grad_outputs=torch.ones_like(T),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "        qw = k*T_y\n",
    "        \n",
    "        return qw\n",
    "\n",
    "    # equation loss prediction\n",
    "    def net_eq(self, x, r):\n",
    "        \"\"\" \n",
    "\n",
    "        Thermal properties involved in equation losses are mean values in the whole domain.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        u, v, p, T, Mt, kt = self.net_pd(x, r)\n",
    "        D, M, k, h = self.prop_pd(T)\n",
    "\n",
    "        Du_x = torch.autograd.grad(\n",
    "            D*u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "        Dv_r = torch.autograd.grad(\n",
    "            D*v, r, \n",
    "            grad_outputs=torch.ones_like(v),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "\n",
    "\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "        u_r = torch.autograd.grad(\n",
    "            u, r, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "\n",
    "        v_x = torch.autograd.grad(\n",
    "            v, x, \n",
    "            grad_outputs=torch.ones_like(v),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0] \n",
    "        v_r = torch.autograd.grad(\n",
    "            v, r, \n",
    "            grad_outputs=torch.ones_like(v),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "\n",
    "        p_x = torch.autograd.grad(\n",
    "            p, x, \n",
    "            grad_outputs=torch.ones_like(p),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "        p_r = torch.autograd.grad(\n",
    "            p, r, \n",
    "            grad_outputs=torch.ones_like(p),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "\n",
    "\n",
    "        divU = u_x + (v_r + v/r)  # divergence\n",
    "        # shear stress \n",
    "        shear_xx = 2*(M+Mt) * (u_x - divU/3)\n",
    "        shear_xr = (M+Mt) * (v_x + u_r)\n",
    "        shear_rr = 2*(M+Mt) * (v_r - divU/3)\n",
    "        shear_xx_x = torch.autograd.grad(\n",
    "            shear_xx, x, \n",
    "            grad_outputs=torch.ones_like(shear_xx),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0] \n",
    "        shear_xr_r = torch.autograd.grad(\n",
    "            r*shear_xr, r, \n",
    "            grad_outputs=torch.ones_like(shear_xr),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "        shear_rx_x = torch.autograd.grad(\n",
    "            shear_xr, x, \n",
    "            grad_outputs=torch.ones_like(shear_xr),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "        shear_rr_r = torch.autograd.grad(\n",
    "            r*shear_rr, r, \n",
    "            grad_outputs=torch.ones_like(shear_rr),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "\n",
    "        T_x = torch.autograd.grad(\n",
    "            T, x, \n",
    "            grad_outputs=torch.ones_like(T),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "        T_r = torch.autograd.grad(\n",
    "            T, r, \n",
    "            grad_outputs=torch.ones_like(T),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "        h_x = torch.autograd.grad(\n",
    "            h, x, \n",
    "            grad_outputs=torch.ones_like(h),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "        h_r = torch.autograd.grad(\n",
    "            h, r, \n",
    "            grad_outputs=torch.ones_like(h),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "        # heat flux\n",
    "        q_x = (k+kt) * T_x\n",
    "        q_r = (k+kt) * T_r\n",
    "        q_xx = torch.autograd.grad(\n",
    "            q_x, x, \n",
    "            grad_outputs=torch.ones_like(q_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "        q_rr = torch.autograd.grad(\n",
    "            r*q_r, r, \n",
    "            grad_outputs=torch.ones_like(q_r),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "            )[0]\n",
    "\n",
    "        eq_m = Du_x + (Dv_r + D*v/r)  # mass conservation\n",
    "        eq_u = D*u*u_x + D*v*u_r + p_x - 1/self.Re * (shear_xx_x + shear_xr_r/r) - D/self.Fr  # momentum conservation at direction of x\n",
    "        eq_v = D*u*v_x + D*v*v_r + p_r - 1/self.Re * (shear_rx_x + shear_rr_r/r - 2*M/r*(v/r - divU/3))  # momentum conservation at direction of r\n",
    "        eq_T = D*u*h_x + D*v*h_r - 1/self.Re/self.Pr * (q_xx + q_rr/r)  # energy conservation\n",
    "        return eq_m, eq_u, eq_v, eq_T\n",
    "\n",
    "    def loss_cal(self):\n",
    "        eq_m, eq_u, eq_v, eq_T = self.net_eq(self.X_gap[:, 0:1], self.X_gap[:, 1:2])  # equation loss\n",
    "        r_qt = self.X_eq[:, 1:2]\n",
    "        u_pred, v_pred, p_pred, T_pred, Mt_pred, kt_pred = self.net_pd(self.X_eq[:, 0:1], r_qt)  # label loss\n",
    "        u_pred_nw, v_pred_nw, p_pred_nw, T_pred_nw, Mt_pred_nw, kt_pred_nw = self.net_pd(self.X_nw[:, 0:1], self.X_nw[:, 1:2])  # loss of wall temperature, not adopted here\n",
    "\n",
    "        # loss weight\n",
    "        l_eq_m, l_eq_u, l_eq_v, l_eq_T = self.loss_weight['loss_eq_m'], self.loss_weight['loss_eq_u'], self.loss_weight['loss_eq_v'], self.loss_weight['loss_eq_T']\n",
    "        l_u_eq, l_v_eq, l_T_eq, l_p_eq = self.loss_weight['loss_u_eq'], self.loss_weight['loss_v_eq'], self.loss_weight['loss_T_eq'], self.loss_weight['loss_p_eq']\n",
    "        l_T_nw = self.loss_weight['loss_T_nw']\n",
    "\n",
    "\n",
    "        loss_eq_m = l_eq_m * torch.mean(eq_m**2)\n",
    "        loss_eq_u = l_eq_u * torch.mean(eq_u**2)\n",
    "        loss_eq_v = l_eq_v * torch.mean(eq_v**2)\n",
    "        loss_eq_T = l_eq_T * torch.mean(eq_T**2)\n",
    "\n",
    "        # when close to the Tpc, increase the weight\n",
    "        idx_Tpc = (abs(self.T_eq - self.T_pc) < 0.03)\n",
    "        weight_pc = 1\n",
    "        loss_u_eq = l_u_eq * (torch.mean(((u_pred - self.u_eq)[~idx_Tpc]**2)) + weight_pc*torch.mean(((u_pred - self.u_eq)[idx_Tpc]**2)))\n",
    "        loss_v_eq = l_v_eq * (torch.mean((1e3*(v_pred - self.v_eq)[~idx_Tpc]**2)) + weight_pc*torch.mean((1e3*(v_pred - self.v_eq)[idx_Tpc]**2)))\n",
    "        loss_T_eq = l_T_eq * (torch.mean(((T_pred - self.T_eq)[~idx_Tpc]**2)) + weight_pc*torch.mean(((T_pred - self.T_eq)[idx_Tpc]**2)))\n",
    "        loss_p_eq = l_p_eq * (torch.mean((1e-2*(p_pred - self.p_eq)[~idx_Tpc]**2)) + weight_pc*torch.mean((1e-2*(p_pred - self.p_eq)[idx_Tpc]**2)))\n",
    "        loss_T_nw = l_T_nw * torch.mean((T_pred_nw - self.T_nw)**2)\n",
    "        loss_u = loss_u_eq + loss_v_eq\n",
    "        loss_p = loss_p_eq\n",
    "        loss_T = loss_T_eq +  loss_T_nw\n",
    "        loss_eq = loss_eq_m + loss_eq_u + loss_eq_v + loss_eq_T \n",
    "        loss = loss_u + loss_T + loss_eq + loss_p\n",
    "\n",
    "        # record loss in tensorboard\n",
    "        writer.add_scalar('Loss_eq_m', loss_eq_m, self.iter)\n",
    "        writer.add_scalar('Loss_eq_u', loss_eq_u, self.iter)\n",
    "        writer.add_scalar('Loss_eq_v', loss_eq_v, self.iter)\n",
    "        writer.add_scalar('Loss_eq_T', loss_eq_T, self.iter)\n",
    "        writer.add_scalar('Loss_T_eq', loss_T_eq, self.iter)\n",
    "        writer.add_scalar('Loss_u_eq', loss_u_eq, self.iter)\n",
    "        writer.add_scalar('Loss_v_eq', loss_v_eq, self.iter)\n",
    "        writer.add_scalar('Loss_p_eq', loss_p_eq, self.iter)\n",
    "        writer.add_scalar('Loss_T_nw', loss_T_nw, self.iter)\n",
    "        writer.add_scalar('Loss_u', loss_u, self.iter)\n",
    "        writer.add_scalar('Loss_T', loss_T, self.iter)\n",
    "        writer.add_scalar('Loss_eq', loss_eq, self.iter)\n",
    "        writer.add_scalar('Loss', loss, self.iter)\n",
    "\n",
    "        # print loss\n",
    "        self.iter += 1\n",
    "        if self.iter % 50 == 0:\n",
    "            print('Iteration %d: loss: %.2e' %(self.iter, loss))\n",
    "            print('loss_eq_m: %.2e, loss_eq_u: %.2e, loss_eq_v: %.2e, loss_eq_T: %.2e' %(loss_eq_m, loss_eq_u, loss_eq_v, loss_eq_T))\n",
    "            print('loss_u_eq: %.2e, loss_v_eq: %.2e, loss_T_eq: %.2e, loss_p_eq: %.2e, loss_T_nw: %.2e' %(loss_u_eq, loss_v_eq, loss_T_eq, loss_p_eq, loss_T_nw))\n",
    "\n",
    "        return loss, loss_u, loss_T, loss_eq\n",
    "\n",
    "    # model training\n",
    "    def train(self, iterat, optimizer_Adam_s, optimizer_Adam_u, optimizer_Adam_T, min_lr, patience):\n",
    "        self.optimizer_Adam_s = optimizer_Adam_s\n",
    "        self.optimizer_Adam_u = optimizer_Adam_u\n",
    "        self.optimizer_Adam_T = optimizer_Adam_T\n",
    "        tic = time.time()\n",
    "        scheduler_s = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer_Adam_s, factor=0.6, patience=patience, min_lr=min_lr)\n",
    "        scheduler_u = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer_Adam_u, factor=0.6, patience=patience, min_lr=min_lr)\n",
    "        scheduler_T = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer_Adam_T, factor=0.6, patience=patience, min_lr=min_lr)\n",
    "        for i in range(iterat):\n",
    "            loss, loss_u, loss_T, loss_eq = self.loss_cal()\n",
    "            \n",
    "            loss_eq.backward(retain_graph=True)\n",
    "            loss_u.backward(retain_graph=True)\n",
    "            self.optimizer_Adam_u.step()\n",
    "\n",
    "            loss_T.backward(retain_graph=False)\n",
    "            self.optimizer_Adam_T.step()\n",
    "            self.optimizer_Adam_s.step()\n",
    " \n",
    "\n",
    "            self.optimizer_Adam_s.zero_grad()\n",
    "            self.optimizer_Adam_u.zero_grad()\n",
    "            self.optimizer_Adam_T.zero_grad()\n",
    "            scheduler_s.step(loss)\n",
    "            scheduler_u.step(loss_u)\n",
    "            scheduler_T.step(loss_T)\n",
    "            writer.add_scalar('Learning rate for s', self.optimizer_Adam_s.state_dict()['param_groups'][0]['lr'], self.iter)\n",
    "            writer.add_scalar('Learning rate for u', self.optimizer_Adam_u.state_dict()['param_groups'][0]['lr'], self.iter)\n",
    "            writer.add_scalar('Learning rate for T', self.optimizer_Adam_T.state_dict()['param_groups'][0]['lr'], self.iter)\n",
    "\n",
    "        toc = time.time()\n",
    "        print('Adam training time: %.2fs' %(toc - tic))\n",
    "        self.loss_epoch.append(float(self.iter))\n",
    "\n",
    "        return\n",
    "\n",
    "    # predict physical fields\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        x, r= X[:, 0:1], X[:, 1:2]\n",
    "        self.dnn_s.eval()\n",
    "        self.dnn_u.eval()\n",
    "        self.dnn_T.eval()\n",
    "        u, v, p, T, Mt, kt = self.net_pd(x, r)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        v = v.detach().cpu().numpy()\n",
    "        p = p.detach().cpu().numpy()\n",
    "        T = T.detach().cpu().numpy()\n",
    "        Mt = Mt.detach().cpu().numpy()\n",
    "        kt = kt.detach().cpu().numpy()\n",
    "        return u, v, p, T, Mt, kt\n",
    "\n",
    "    # predict thermal properties\n",
    "    def predict_prop(self, T):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        T   ::  array-like or float, dimensionless Temperature\n",
    "\n",
    "        Return:\n",
    "        D   ::  array-like or float, dimensionless density\n",
    "        M   ::  array-like or float, dimensionless viscosity\n",
    "        k   ::  array-like or float, dimensionless conductivity\n",
    "        h   ::  array-like or float, dimensionless enthalpy\n",
    "        \"\"\"\n",
    "\n",
    "        T = torch.tensor(T, requires_grad=True).float().to(device)\n",
    "        D, M, k, h = self.prop_pd(T)\n",
    "        D = D.detach().cpu().numpy()\n",
    "        M = M.detach().cpu().numpy()\n",
    "        k = k.detach().cpu().numpy()\n",
    "        h = h.detach().cpu().numpy()\n",
    "        return D, M, k, h\n",
    "\n",
    "    # predict heat flux \n",
    "    def predict_qw(self, X):\n",
    "        X = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        x, r= X[:, 0:1], X[:, 1:2]\n",
    "        self.dnn_s.eval()\n",
    "        self.dnn_u.eval()\n",
    "        self.dnn_T.eval()\n",
    "        u, v, p, T, Mt, kt = self.net_pd(x, r)\n",
    "        qw = self.qw_pd(T, r)\n",
    "        qw = qw.detach().cpu().numpy()\n",
    "        return qw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thermal property prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_prop = [1, 20, 20, 20, 20, 20, 4]\n",
    "prop_net = DNN(layers_prop)\n",
    "prop_net.load_state_dict(torch.load(os.path.join(basepath, model_path, 'Prop_6layers(20)_6props_0121_8800kPa_CO2.pkl')))\n",
    "Tlb, Tub = 200, 600  # CO2\n",
    "\n",
    "prop_cof = [1e3, 1e-4, 1e-1, 1e5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluid = 'CO2'\n",
    "Tpc = 312.11536057680286\n",
    "Tref = 800\n",
    "L0, T0, Re0, p_in = 0.496e-3, 297.55, 1900.0, 8.8e6\n",
    "D0 = property_from_NIST('Dmass', T0, p_in, fluid)\n",
    "Cp0 = property_from_NIST('Cpmass', T0, p_in, fluid)\n",
    "M0 = property_from_NIST('viscosity', T0, p_in, fluid)\n",
    "k0 = property_from_NIST('conductivity', T0, p_in, fluid)\n",
    "U0 = Re0*M0/(D0*L0)\n",
    "hpc = property_from_NIST('Hmass', Tpc, p_in, fluid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "data_path_dns = [\n",
    "    '02 - data/DNS_data/CaoYuli/P8.8q549d99.mat',\n",
    "    ]\n",
    "\n",
    "N_wc_dns = len(data_path_dns)\n",
    "dns_data = pd.DataFrame()\n",
    "\n",
    "data_2d_dns = {}\n",
    "data_0d_dns, data_1d_dns = pd.DataFrame(), pd.DataFrame()\n",
    "data_2d_wc = {}\n",
    "dns_temp = scipy.io.loadmat(os.path.join(basepath, data_path_dns[idx]))\n",
    "\n",
    "r_dns = dns_temp['y'].reshape(-1)\n",
    "y_dns = 1 - r_dns\n",
    "x_dns = 2*dns_temp['x'].reshape(-1)\n",
    "\n",
    "\n",
    "drop_feats = ['__header__', '__version__', '__globals__', 'A', 'ans', 'N1', 'N2', 'Nk', 'NX', 'NY', 'R_IO', 'ReN', 'str', 'time1', 'time2', 'table', 'temp', 'data1', 'data2', 'data3', 'data4', 'dummy_thel',  'dummy1_thel', 'dummy2_thel', 'dummyline', 'fid1', 'fid2', 'fid3', 'fid4',  'RLEMPI1_thel', 'RLEMPI2_thel', 'i', 'j', 'k', 'J', 'y', 'xx', 'xND', 'len', 'dummy', 'dummy1', 'dummy2', 'RLEMPI1', 'RLEMPI2', 'a', 'dummy1_flow', 'dummy_flow', 'dummy2_flow', 'fid5', 'fid6', 'fid7', 'RLEMPI1_FLOW', 'RLEMPI2_FLOW']\n",
    "for d_f in drop_feats:\n",
    "    if d_f in dns_temp.keys():\n",
    "        del dns_temp[d_f]\n",
    "\n",
    "# Classify variables with different dimension\n",
    "data_0d_temp, data_1d_temp = pd.DataFrame(np.zeros(1)), pd.DataFrame(np.zeros(len(dns_temp['x'][0])))\n",
    "for feat,data in dns_temp.items():\n",
    "    # Scalar\n",
    "    if data.shape[1] == 1:\n",
    "        data_0d_temp[feat] = data[0][0]\n",
    "\n",
    "    # 1d-array\n",
    "    elif data.shape[0] == 1:\n",
    "        data_1d_temp[feat] = data[0]\n",
    "    \n",
    "    # 2d-array\n",
    "    elif len(data.shape) == 2:\n",
    "        data_2d_wc[feat] = pd.DataFrame(data)\n",
    "        data_2d_wc[feat].columns = r_dns\n",
    "\n",
    "data_0d_temp['P0'] = p_in\n",
    "data_0d_temp['Re0'] = Re0\n",
    "data_0d_temp['U0'] = data_0d_temp['Re0'] * data_0d_temp['M0'] / data_0d_temp['D0'] / (2*data_0d_temp['L0'])\n",
    "data_0d_temp['h0'] = property_from_NIST('Hmass', data_0d_temp.loc[0, 'T0'], data_0d_temp.loc[0, 'P0'], 'CO2')\n",
    "data_0d_temp['hpc'] = property_from_NIST('Hmass', Tpc, data_0d_temp.loc[0, 'P0'], 'CO2')\n",
    "data_0d_temp['href'] = property_from_NIST('Hmass', Tref, data_0d_temp.loc[0, 'P0'], 'CO2')\n",
    "data_0d_temp.drop(columns=0, inplace=True)\n",
    "data_1d_temp.drop(columns=0, inplace=True)\n",
    "data_0d_temp['wc_idx'] = idx\n",
    "data_1d_temp['wc_idx'] = idx\n",
    "\n",
    "data_1d_temp.insert(0, 'x/r', 2*data_1d_temp['x'])\n",
    "data_1d_temp.drop(columns='x', inplace=True)\n",
    "data_1d_temp['Tw'] = data_2d_wc['T_Re'].iloc[:, -1].values\n",
    "\n",
    "data_0d_dns = pd.concat((data_0d_dns, data_0d_temp))\n",
    "data_1d_dns = pd.concat((data_1d_dns, data_1d_temp))\n",
    "data_2d_dns[idx] = data_2d_wc\n",
    "    \n",
    "data_0d_dns.reset_index(drop=True, inplace=True)\n",
    "data_1d_dns.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d data\n",
    "x_star_dns = data_1d_dns['x/r'].unique()[:, None]  # Nx x 1\n",
    "r_star_dns = r_dns[:,None]  # Ny x 1\n",
    "X_dns, R_dns = np.meshgrid(x_star_dns, r_star_dns, indexing='ij')  # Nx x Ny\n",
    "X_star_dns = np.hstack((X_dns.flatten('F')[:,None], R_dns.flatten('F')[:,None]))  # NxNyNq x 3\n",
    "\n",
    "\n",
    "data_2d_dns = data_2d_dns[idx]\n",
    "u_dns = data_2d_dns['Ux_Re'].to_numpy()  # Nx x Ny\n",
    "v_dns = data_2d_dns['Uy_Re'].to_numpy()  # Nx x Ny\n",
    "p_dns = data_2d_dns['P_Re'].to_numpy()  # Nx x Ny\n",
    "T_dns = data_2d_dns['T_Re'].to_numpy()  # Nx x Ny\n",
    "D_dns = data_2d_dns['D_Re'].to_numpy()  # Nx x Ny\n",
    "M_dns = data_2d_dns['M_Re'].to_numpy()  # Nx x Ny\n",
    "k_dns = data_2d_dns['K_Re'].to_numpy()  # Nx x Ny\n",
    "# the original temperature of enthalpy is 800K\n",
    "h_dns = data_2d_dns['H_Re'].to_numpy() + (data_0d_dns.loc[0, 'href']-data_0d_dns.loc[0, 'hpc'])/(data_0d_dns.loc[0, 'T0']*data_0d_dns.loc[0, 'Cp0']) # Nx x Ny\n",
    "\n",
    "u_star_dns = u_dns.flatten('F')[:,None]  # NxNyNq x 1\n",
    "v_star_dns = v_dns.flatten('F')[:,None]  # NxNyNq x 1\n",
    "p_star_dns = p_dns.flatten('F')[:,None]  # NxNyNq x 1\n",
    "h_star_dns = h_dns.flatten('F')[:,None]  # NxNyNq x 1\n",
    "T_star_dns = T_dns.flatten('F')[:,None]  # NxNyNq x 1\n",
    "D_star_dns = D_dns.flatten('F')[:,None]  # NxNyNq x 1\n",
    "M_star_dns = M_dns.flatten('F')[:,None]  # NxNyNq x 1\n",
    "k_star_dns = k_dns.flatten('F')[:,None]  # NxNyNq x 1\n",
    "\n",
    "U_star_dns = np.array([])\n",
    "U_star_dns = np.hstack((u_star_dns, v_star_dns, T_star_dns, D_star_dns, M_star_dns, k_star_dns, h_star_dns, p_star_dns))  # NxNyNq x 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "set_seed(1)\n",
    "\n",
    "# training set\n",
    "N_eq = 1000  # number of data for label loss\n",
    "N_gap = 300  # number of data for equation loss\n",
    "lb_dns = np.array([0.0, 0.0])  # lower bound for input coordinates\n",
    "ub_dns = np.array([300.0, 1.0])  # upper bound for input coordinates\n",
    "\n",
    "\n",
    "X_eq_dns, U_eq_dns = np.array([]), np.array([])\n",
    "X_train = X_star_dns[(X_star_dns[:, 0] <= 297)]\n",
    "U_train = U_star_dns[(X_star_dns[:, 0] <= 297)]\n",
    "\n",
    "# For more uniform sampling, 30% data are in the near-wall region (r >= 0.8)\n",
    "wall_bd = 0.8\n",
    "test_pc = 0.3\n",
    "X_core_train = X_train[((X_train[:, 1] < wall_bd))]\n",
    "X_wall_train = X_train[(X_train[:, 1] >= wall_bd)]\n",
    "U_core_train = U_train[((X_train[:, 1] < wall_bd))]\n",
    "U_wall_train = U_train[(X_train[:, 1] >= wall_bd)]\n",
    "idx_eq_core = np.random.choice(X_core_train.shape[0], int(N_eq*(1-test_pc)), replace=False)\n",
    "idx_eq_wall = np.random.choice(X_wall_train.shape[0], int(N_eq*test_pc), replace=False)\n",
    "X_eq_dns = np.vstack((X_core_train[idx_eq_core], X_wall_train[idx_eq_wall]))\n",
    "U_eq_dns = np.vstack((U_core_train[idx_eq_core], U_wall_train[idx_eq_wall]))\n",
    "\n",
    "X_core_test = X_train[((X_train[:, 1] < wall_bd))]\n",
    "X_wall_test = X_train[(X_train[:, 1] >= wall_bd)]\n",
    "U_core_test = U_train[((X_train[:, 1] < wall_bd))]\n",
    "U_wall_test = U_train[(X_train[:, 1] >= wall_bd)]\n",
    "idx_eq_core = np.random.choice(X_core_test.shape[0], int(N_gap*(1-test_pc)), replace=False)\n",
    "idx_eq_wall = np.random.choice(X_wall_test.shape[0], int(N_gap*test_pc), replace=False)\n",
    "X_gap_dns = np.vstack((X_core_test[idx_eq_core], X_wall_test[idx_eq_wall]))\n",
    "U_gap_dns = np.vstack((U_core_test[idx_eq_core], U_wall_test[idx_eq_wall]))\n",
    "\n",
    "\n",
    "# wall temperature sampling\n",
    "## N_nw is the number of data sampled in the direction of r\n",
    "## N_b is the number of data sampled in the direction of x\n",
    "N_nw, N_b = 1, 10 \n",
    "idx_b = np.array([int(x) for x in np.linspace(0, x_dns.shape[0]-1, N_b)])   # uniform distribution\n",
    "for j in range(N_b):\n",
    "    X_temp = X_star_dns[(X_star_dns[:, 0]==x_dns[idx_b[j]])&(X_star_dns[:, 1]>=r_dns[-N_nw])]\n",
    "    T_temp = U_star_dns[(X_star_dns[:, 0]==x_dns[idx_b[j]])&(X_star_dns[:, 1]>=r_dns[-N_nw]), 2][:,None]\n",
    "    if j == 0:\n",
    "        X_nw_dns = X_temp\n",
    "        T_nw_dns = T_temp\n",
    "    else:\n",
    "        X_nw_dns = np.vstack((X_nw_dns, X_temp))\n",
    "        T_nw_dns = np.vstack((T_nw_dns, T_temp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Network construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN size\n",
    "layers_s = [2, 36, 36, 36, 36]\n",
    "dnn_s = DNN(layers_s)\n",
    "\n",
    "layers_u = [24, 24, 24, 4]\n",
    "dnn_u = DNN(layers_u)\n",
    "\n",
    "layers_T = [12, 12, 12, 2]\n",
    "dnn_T = DNN(layers_T)\n",
    "\n",
    "# Initialize or load the model\n",
    "LOADMODEL = True\n",
    "# LOADMODEL = False\n",
    "if LOADMODEL:\n",
    "    dnn_s.load_state_dict(torch.load(os.path.join(basepath, model_path, '2-head_s 8layers(36)_THCS.pkl')))\n",
    "    dnn_u.load_state_dict(torch.load(os.path.join(basepath, model_path, '2-head_u 8layers(24)_THCS.pkl')))\n",
    "    dnn_T.load_state_dict(torch.load(os.path.join(basepath, model_path, '2-head_T 8layers(12)_THCS.pkl')))\n",
    "else:\n",
    "    dnn_s.apply(initialize_net)\n",
    "    dnn_u.apply(initialize_net)\n",
    "    dnn_T.apply(initialize_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weight = {\n",
    "    'loss_eq_m':1e4, 'loss_eq_u':1e4, 'loss_eq_v':1e3, 'loss_eq_T':1e6, \n",
    "    'loss_u_ub':1e0, 'loss_v_ub':1e0, 'loss_T_ub':1e0,\n",
    "    'loss_u_lb':1e0, 'loss_v_lb':1e0, 'loss_T_lb':1e0,\n",
    "    'loss_u_0':1e0, 'loss_v_0':1e0, 'loss_T_0':1e0,\n",
    "    'loss_u_eq':1e5, 'loss_v_eq':1e4, 'loss_T_eq':1e7, 'loss_p_eq':0, 'loss_T_nw':0,\n",
    "    'loss_qw':0,\n",
    "    }\n",
    "thcs = THCS(X_eq_dns, U_eq_dns, X_gap_dns, U_gap_dns, X_nw_dns, T_nw_dns, dnn_s, dnn_u, dnn_T, prop_net, lb=lb_dns, ub=ub_dns, loss_weight=loss_weight, data_0d=data_0d_dns, T_lb=Tlb, T_ub=Tub, prop_cof=prop_cof, Tpc=Tpc/T0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "num_epochs = 40000\n",
    "thcs.train(num_epochs, optim.Adam(dnn_s.parameters(), lr=lr), optim.Adam(dnn_u.parameters(), lr=lr), optim.Adam(dnn_T.parameters(), lr=lr), min_lr=6e-3, patience=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dnn_u: u, v, p, Mt; dnn_T: T, kt.\n",
    "# torch.save(dnn_s.state_dict(), os.path.join(basepath, model_path, '2-head_s 8layers(36)_THCS.pkl'))\n",
    "# torch.save(dnn_u.state_dict(), os.path.join(basepath, model_path, '2-head_u 8layers(24)_THCS.pkl'))\n",
    "# torch.save(dnn_T.state_dict(), os.path.join(basepath, model_path, '2-head_T 8layers(12)_THCS.pkl'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2d_wc = {}\n",
    "feats_keys = {'Ux_Re':'u', 'Uy_Re':'v', 'P_Re':'p', 'T_Re':'T', 'D_Re':'D', 'M_Re':'M', 'K_Re':'k', 'H_Re':'h'}\n",
    "for k,v in feats_keys.items():\n",
    "    data_2d_wc[v] = data_2d_dns[k]\n",
    "    \n",
    "u = data_2d_wc['u'].to_numpy()  # Nx x Ny\n",
    "v = data_2d_wc['v'].to_numpy()  # Nx x Ny\n",
    "p = data_2d_wc['p'].to_numpy()  # Nx x Ny\n",
    "T = data_2d_wc['T'].to_numpy()  # Nx x Ny\n",
    "D = data_2d_wc['D'].to_numpy()  # Nx x Ny\n",
    "M = data_2d_wc['M'].to_numpy()  # Nx x Ny\n",
    "k = data_2d_wc['k'].to_numpy()  # Nx x Ny\n",
    "h = data_2d_wc['h'].to_numpy()  # Nx x Ny\n",
    "\n",
    "X_star_draw = X_star_dns\n",
    "u_pred, v_pred, p_pred, T_pred, Mt_pred, kt_pred = thcs.predict(X_star_draw[X_star_draw[:, 0] <= 297, :])\n",
    "D_pred, M_pred, k_pred, h_pred = thcs.predict_prop(T_pred)\n",
    "uu_pred = griddata(X_star_draw[X_star_draw[:, 0] <= 297,:2], u_pred.flatten(), (X_dns[X_dns[:, 0] <= 297, :], R_dns[X_dns[:, 0] <= 297, :]), method='cubic')\n",
    "vv_pred = griddata(X_star_draw[X_star_draw[:, 0] <= 297,:2], v_pred.flatten(), (X_dns[X_dns[:, 0] <= 297, :], R_dns[X_dns[:, 0] <= 297, :]), method='cubic')\n",
    "pp_pred = griddata(X_star_draw[X_star_draw[:, 0] <= 297,:2], p_pred.flatten(), (X_dns[X_dns[:, 0] <= 297, :], R_dns[X_dns[:, 0] <= 297, :]), method='cubic')\n",
    "TT_pred = griddata(X_star_draw[X_star_draw[:, 0] <= 297,:2], T_pred.flatten(), (X_dns[X_dns[:, 0] <= 297, :], R_dns[X_dns[:, 0] <= 297, :]), method='cubic')\n",
    "MtMt_pred = griddata(X_star_draw[X_star_draw[:, 0] <= 297,:2], Mt_pred.flatten(), (X_dns[X_dns[:, 0] <= 297, :], R_dns[X_dns[:, 0] <= 297, :]), method='cubic')\n",
    "DD_pred = griddata(X_star_draw[X_star_draw[:, 0] <= 297,:2], D_pred.flatten(), (X_dns[X_dns[:, 0] <= 297, :], R_dns[X_dns[:, 0] <= 297, :]), method='cubic')\n",
    "MM_pred = griddata(X_star_draw[X_star_draw[:, 0] <= 297,:2], M_pred.flatten(), (X_dns[X_dns[:, 0] <= 297, :], R_dns[X_dns[:, 0] <= 297, :]), method='cubic')\n",
    "kk_pred = griddata(X_star_draw[X_star_draw[:, 0] <= 297,:2], k_pred.flatten(), (X_dns[X_dns[:, 0] <= 297, :], R_dns[X_dns[:, 0] <= 297, :]), method='cubic')\n",
    "hh_pred = griddata(X_star_draw[X_star_draw[:, 0] <= 297,:2], h_pred.flatten(), (X_dns[X_dns[:, 0] <= 297, :], R_dns[X_dns[:, 0] <= 297, :]), method='cubic')\n",
    "e_T = abs((TT_pred - T[X_dns[:, 0] <= 297, :])/T[X_dns[:, 0] <= 297, :]).mean()*100\n",
    "e_u = abs((uu_pred - u[X_dns[:, 0] <= 297, :])/u[X_dns[:, 0] <= 297, :]).mean()*100\n",
    "e_v = abs((vv_pred - v[X_dns[:, 0] <= 297, :])/v[X_dns[:, 0] <= 297, :]).mean()*100\n",
    "l_T = (abs(TT_pred - T[X_dns[:, 0] <= 297, :])).mean()\n",
    "l_u = (abs(uu_pred - u[X_dns[:, 0] <= 297, :])).mean()\n",
    "l_v = (abs(vv_pred - v[X_dns[:, 0] <= 297, :])).mean()\n",
    "l_T = (abs(TT_pred - T[X_dns[:,0]<297, :])).mean()\n",
    "l_u = (abs(uu_pred - u[X_dns[:,0]<297, :])).mean()\n",
    "l_v = (abs(vv_pred - v[X_dns[:,0]<297, :])).mean()\n",
    "l_p = (abs(pp_pred - p[X_dns[:,0]<297, :])).mean()\n",
    "print('Relative error: error_u = %.2f%%, error_v = %.2f%%,  error_T = %.2f%%' %(e_u, e_v, e_T))\n",
    "print('Loss: Loss_u = %.2e, Loss_v = %.2e,  Loss_T = %.2e' %(l_u, l_v, l_T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b6cc9fe08dcdbd6e7b45b8f32c32bad7aa592cdbb2dd061fb72a7e00a5176cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
